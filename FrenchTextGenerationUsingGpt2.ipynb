{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bakeita/Machine-Learning-Models/blob/main/FrenchTextGenerationUsingGpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqGxDi0xoqLP",
        "outputId": "76b49f41-1ef1-4bec-a586-58d6b7465c5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "UTI2Eql5-r4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using the pretrained gpt2 model for french text generation\n"
      ],
      "metadata": {
        "id": "paQJupI1vooD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2LMHeadModel.from_pretrained(\"asi/gpt-fr-cased-base\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"asi/gpt-fr-cased-base\")\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Th64mKPZqLTu",
        "outputId": "17cb0fdc-43f7-4006-c431-d5c9a0a50b7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50000, 1792)\n",
              "    (wpe): Embedding(1024, 1792)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0): GPT2Block(\n",
              "        (ln_1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): GPT2Block(\n",
              "        (ln_1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): GPT2Block(\n",
              "        (ln_1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): GPT2Block(\n",
              "        (ln_1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): GPT2Block(\n",
              "        (ln_1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): GPT2Block(\n",
              "        (ln_1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): GPT2Block(\n",
              "        (ln_1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): GPT2Block(\n",
              "        (ln_1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): GPT2Block(\n",
              "        (ln_1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): GPT2Block(\n",
              "        (ln_1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): GPT2Block(\n",
              "        (ln_1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): GPT2Block(\n",
              "        (ln_1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (12): GPT2Block(\n",
              "        (ln_1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (13): GPT2Block(\n",
              "        (ln_1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (14): GPT2Block(\n",
              "        (ln_1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (15): GPT2Block(\n",
              "        (ln_1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (16): GPT2Block(\n",
              "        (ln_1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (17): GPT2Block(\n",
              "        (ln_1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (18): GPT2Block(\n",
              "        (ln_1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (19): GPT2Block(\n",
              "        (ln_1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (20): GPT2Block(\n",
              "        (ln_1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (21): GPT2Block(\n",
              "        (ln_1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (22): GPT2Block(\n",
              "        (ln_1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (23): GPT2Block(\n",
              "        (ln_1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1792, out_features=50000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Greedy Search**\n",
        "\n",
        "Greedy search simply selects the word with the highest probability as its next word: $w_t = argmax_{w}P(w | w_{1:t-1})$ at each timestep $t$. The following sketch shows greedy search. \n",
        "\n",
        "![Greedy Search](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/greedy_search.png)\n",
        "\n",
        "Starting from the word $\\text{\"longtemps\"}$, the algorithm \n",
        "greedily chooses the next word of highest probability $\\text{\"je\"}$ and so on, so that the final generated word sequence is $\\text{\"Longtemps\", \"je\", \"me\"}$ having an overall probability of $0.5 \\times 0.4 = 0.2$.\n",
        "\n",
        "In the following we will generate word sequences using GPT2 on the context $(\\text{\"Longtemps\", \"je\", \"me\", 'suis', \"couch√©\", \"de\", \"bonne\", \"heure.\"})$. Let's see how greedy search can be used in `transformers` as follows:\n"
      ],
      "metadata": {
        "id": "2RTVzjdSmWaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = \"Longtemps je me suis couch√© de bonne heure.\"\n",
        "input_ids = tokenizer.encode(input_sentence, return_tensors='pt')\n",
        "\n",
        "greedy_outputs = model.generate(\n",
        "    input_ids, \n",
        "    max_length=100\n",
        ")\n",
        "\n",
        "print(50 * '-'+\"Output:\\n\" + 50 * '-')\n",
        "print(tokenizer.decode(greedy_outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTXhevZvmV3q",
        "outputId": "1e083923-c9a7-4c22-a530-0afda587d9f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Longtemps je me suis couch√© de bonne heure. J'avais une bonne raison de dormir. Je me suis lev√© √† cinq heures, j'ai pris mon petit d√©jeuner, j'ai d√©jeun√©, j'ai d√Æn√©, j'ai d√Æn√©, j'ai d√Æn√©, j'ai d√Æn√©, j'ai d√Æn√©, j'ai d√Æn√©, j'ai d√Æn√©, j'ai d√Æn√©, j'ai d√Æn√©, j'ai d√Æn√©\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Beam search**\n",
        "\n",
        "reduces the risk of missing hidden high probability word sequences by keeping the most likely `num_beams` of hypotheses at each time step and eventually choosing the hypothesis that has the overall highest probability. Let's illustrate with `num_beams=2`:"
      ],
      "metadata": {
        "id": "VsOrkec2o1oq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = \"Longtemps je me suis couch√© de bonne heure.\"\n",
        "input_ids = tokenizer.encode(input_sentence, return_tensors='pt')\n",
        "\n",
        "beam_outputs = model.generate(\n",
        "    input_ids, \n",
        "    max_length=50, \n",
        "    num_beams=5, \n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "print(50 * '-'+\"Output:\\n\" + 50 * '-')\n",
        "print(tokenizer.decode(beam_outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umSRMaFIpXqO",
        "outputId": "10ff4303-0f6f-4936-cd4e-1272b1b01b9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Longtemps je me suis couch√© de bonne heure. J‚Äôavais pris l‚Äôhabitude de m‚Äô√©tendre sur mon lit, de me recroqueviller sur moi-m√™me, de me recroqueviller sur moi-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice, that looks much better! We can see that the repetition does not appear anymore. Nevertheless, n-gram penalties have to be used with care. An article generated about the city New York should not use a 2-gram penalty or otherwise, the name of the city would only appear once in the whole text!\n",
        "\n",
        "Another important feature about beam search is that we can compare the top beams after generation and choose the generated beam that fits our purpose best.\n",
        "\n",
        "In transformers, we simply set the parameter num_return_sequences to the number of highest scoring beams that should be returned. Make sure though that num_return_sequences <= num_beams!\n"
      ],
      "metadata": {
        "id": "dneEFy6gpvVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "beam_outputs = model.generate(\n",
        "    input_ids, \n",
        "    max_length=50, \n",
        "    num_beams=5, \n",
        "    no_repeat_ngram_size=2, \n",
        "    num_return_sequences=5, \n",
        "    early_stopping=True\n",
        ")\n",
        "# now we have 5 output sequences\n",
        "print(50 * '-'+\"Output:\" + 50 * '-')\n",
        "for i, beam_output in enumerate(beam_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEIQV7r2p5Y1",
        "outputId": "d2b66e99-a77f-4295-fa3a-502a27e1c277"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------Output:\n",
            "--------------------------------------------------\n",
            "0: je vient de l'ecode, maintenant je dois dire que j'ai eu un peu de mal au d√©but, mais je me suis vite rendu compte que c'etait pas si mal que ca. J'espere que vous allez\n",
            "1: je vient de l'ecode, maintenant je dois dire que j'ai eu un peu de mal au d√©but, mais je me suis vite rendu compte que c'etait pas si mal que ca. J'espere qu'il\n",
            "2: je vient de l'ecode, maintenant je dois dire que j'ai eu un peu de mal au d√©but, mais je me suis vite rendu compte que c'etait pas si mal que ca. J'espere que tu vas\n",
            "3: je vient de l'ecode, maintenant je dois dire que j'ai eu un peu de mal au d√©but, mais je me suis vite rendu compte que c'etait pas si mal que ca. J'espere qu'un\n",
            "4: je vient de l'ecode, maintenant je dois dire que j'ai eu un peu de mal au d√©but, mais je me suis vite rendu compte que c'etait pas si mal que ca. J'espere que vous avez\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sampling**\n",
        "\n",
        "In its most basic form, sampling means randomly picking the next word $w_t$ according to its conditional probability distribution:"
      ],
      "metadata": {
        "id": "_VnCfJ1PqD1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a sample of text\n",
        "input_sentence = \"je vient de l'ecode, maintenant je dois\"\n",
        "input_ids = tokenizer.encode(input_sentence, return_tensors='pt')\n",
        "\n",
        "sampling_outputs = model.generate(\n",
        "    input_ids, \n",
        "    max_length=70, \n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "print(50 * '-'+\"Output:\" + 50 * '-')\n",
        "print(tokenizer.decode(sampling_outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evC1FeKlqFs2",
        "outputId": "29515d07-6194-4c2f-bcc7-cfb21dd71cc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------Output:--------------------------------------------------\n",
            "je vient de l'ecode, maintenant je dois me proteger, et mes deux beaux parents sont partis chez le veto, ma tante et moi, j'aime pas aller a l'ecole, j'ai dit bonne chance l'an dernier, merci beaucoup de faire de belles rencontres, de vous etes vraiment tr√®s genereuse,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Top-K Sampling**\n",
        "\n",
        "[Fan et. al (2018)](https://arxiv.org/pdf/1805.04833.pdf) introduced a simple, but very powerful sampling scheme, called ***Top-K*** sampling. In *Top-K* sampling, the *K* most likely next words are filtered and the probability mass is redistributed among only those *K* next words. \n",
        "GPT2 adopted this sampling scheme, which was one of the reasons for its success in story generation. \n",
        "\n",
        "We extend the range of words used for both sampling steps in the example above from 3 words to 10 words to better illustrate *Top-K* sampling.\n",
        "\n",
        "![top_k_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/top_k_sampling.png)\n",
        "\n",
        "Having set $K = 6$, in both sampling steps we limit our sampling pool to 6 words. While the 6 most likely words, defined as $V_{\\text{top-K}}$ encompass only *ca.* two-thirds of the whole probability mass in the first step, it includes almost all of the probability mass in the second step. Nevertheless, we see that it successfully eliminates the rather weird candidates $\\text{\"not\", \"the\", \"small\", \"told\"}$ \n",
        "in the second sampling step.\n",
        "\n",
        "\n",
        "Let's see how *Top-K* can be used in the library by setting `top_k=50`:"
      ],
      "metadata": {
        "id": "y5nEmoDirXus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a sample of text\n",
        "input_sentence = \"Longtemps je me suis couch√© de bonne heure.\"\n",
        "input_ids = tokenizer.encode(input_sentence, return_tensors='pt')\n",
        "\n",
        "topK_outputs = model.generate(\n",
        "    input_ids, \n",
        "    max_length=90, \n",
        "    do_sample=True,   \n",
        "    top_k=50, \n",
        "    top_p=0.95, \n",
        "    num_return_sequences=1\n",
        ")\n",
        "\n",
        "print(50 * '-'+\"Output:\" + 50 * '-')\n",
        "print(tokenizer.decode(topK_outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nc9E_OPbqsB3",
        "outputId": "2e2e05da-58bc-4e5b-8d44-6e3599fb9844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------Output:--------------------------------------------------\n",
            "Longtemps je me suis couch√© de bonne heure. Pour ne pas me r√©veiller chaque soir je me mettais toujours sur le ventre, et je me sentais fatigu√© si le coeur me battait ou s'il voulait me faire sentir malade. Quand je me r√©veillais je m'appelais Fran√ßois et je m'endormais de bonne heure. Je n'avais pas un sommeil bien agit√©, je n'avais rien √† dire. Mais quelquefois quand je me\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Top-p (nucleus) sampling**\n",
        "\n",
        "Instead of sampling only from the most likely *K* words, in *Top-p* sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability *p*. The probability mass is then redistributed among this set of words. This way, the size of the set of words (*a.k.a* the number of words in the set) can dynamically increase and decrease according to the next word's probability distribution. Ok, that was very wordy, let's visualize.\n",
        "\n",
        "![top_p_sampling](https://github.com/patrickvonplaten/scientific_images/blob/master/top_p_sampling.png?raw=true)\n",
        "\n",
        "Having set $p=0.92$, *Top-p* sampling picks the *minimum* number of words to exceed together $p=92\\%$ of the probability mass, defined as $V_{\\text{top-p}}$. In the first example, this included the 9 most likely words, whereas it only has to pick the top 3 words in the second example to exceed 92%. Quite simple actually! It can be seen that it keeps a wide range of words where the next word is arguably less predictable, *e.g.* $P(w | \\text{\"The\"})$, and only a few words when the next word seems more predictable, *e.g.* $P(w | \\text{\"The\", \"car\"})$.\n",
        "\n",
        "Alright, time to check it out in `transformers`!\n",
        "We activate *Top-p* sampling by setting `0 < top_p < 1`:"
      ],
      "metadata": {
        "id": "QN_pwwJZr-rr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a sample of text\n",
        "input_sentence = \"Longtemps je me suis couch√© de bonne heure.\"\n",
        "input_ids = tokenizer.encode(input_sentence, return_tensors='pt')\n",
        "\n",
        "neucleus_outputs = model.generate(\n",
        "    input_ids, \n",
        "    max_length=90, \n",
        "    do_sample=True,   \n",
        "    top_k=40, \n",
        "    top_p=0.95, \n",
        "    num_return_sequences=1\n",
        ")\n",
        "\n",
        "print(50 * '-'+\"Output:\" + 50 * '-')\n",
        "print(tokenizer.decode(neucleus_outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZZE21DUsBty",
        "outputId": "01fca6ee-791f-4d55-88df-b72f523b2f5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------Output:--------------------------------------------------\n",
            "Longtemps je me suis couch√© de bonne heure. Aujourd‚Äôhui, vers six heures du matin, comme le soleil se couchait encore, je me suis lev√© pour regarder sous le bassin de l‚Äôentr√©e. J‚Äôai ferm√© la porte √† clef derri√®re moi et je suis mont√© √† l‚Äô√©chelle. J‚Äôavais un pantalon d√©chir√© √† la taille et j‚Äôavais l‚Äôair d‚Äôun homme qui\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Here is the final models with all the decoding methods mentionned above\n",
        "\n",
        "   * Greedy Search\n",
        "   * Top-P (nucleus) sampling\n",
        "   * Top-K Sampling\n",
        "   * Sampling\n",
        "   * Beam search \n",
        "\n",
        "And the French gpt-2 is called:  **asi/gpt-fr-cased-base \\n**\n",
        "\n",
        "\n",
        "  GPT-fr üá´üá∑ is a GPT model for French developped by Quantmetry and the Laboratoire de Linguistique Formelle (LLF). We train the model on a very large and heterogeneous French corpus. We release the weights for the following configurations:"
      ],
      "metadata": {
        "id": "v8vpot3vuJ1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a sample of text\n",
        "input_sentence = \"Le laboureur et ses enfants travailler prener de la peine c'est\"\n",
        "input_ids = tokenizer.encode(input_sentence, return_tensors='pt')\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    max_length=100, \n",
        "    do_sample=True,   \n",
        "    top_k=50, \n",
        "    top_p=0.95, \n",
        "    num_return_sequences=1\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwBChGqij7s5",
        "outputId": "dbf888a7-0a50-420b-8316-f3aefc6ebbee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Le laboureur et ses enfants travailler prener de la peine c'est parcequ'il a eleve ses enfants plus que ceux qui n'ont jamais travaille de leur vie. c'est le travail qui leur coute.il doit donc bien travailler, c'est de lui qu'il faut le remercier, et pour cela, il faut lui rendre des comptes, lui dire qu'il doit faire le travail qu'il a fait, le remercier, lui donner ses benedictions\n"
          ]
        }
      ]
    }
  ]
}